%% The first command in your LaTeX source must be the \documentclass command.
%%
%% Options:
%% twocolumn : Two column layout.
%% hf: enable header and footer.
\documentclass[
twocolumn,
% hf,
]{ceurart}

%%
%% One can fix some overfulls
\sloppy

%%
%% Minted listings support 
%% Need pygment <http://pygments.org/> <http://pypi.python.org/pypi/Pygments>
\usepackage{listings}
\usepackage{bm}
\usepackage{url}
\usepackage{array,multirow,graphicx}
\usepackage{float}

%% auto break lines
\lstset{breaklines=true}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% Rights management information.
%% CC-BY is default license.
\copyrightyear{2022}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0).}

%%
%% This command is for the conference information
\conference{COMHUM 2022: Workshop on Computational Methods in the Humanities,
  June 9--10, 2022, Lausanne, Switzerland}

%%
%% The "title" command^
\title{A framework for embedding entities in a textual narrative: a case study on Les Misérables}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
\author[1]{Guillaume Guex}[%s
orcid=0000-0003-1001-9525,
email=guillaume.guex@unil.ch]
\address[1]{Departement of Language and Information Sciences, University of Lausanne, Switzerland}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  A clear and well-documented \LaTeX{} document is presented as an
  article formatted for publication by CEUR-WS in a conference
  proceedings. Based on the ``ceurart'' document class, this article
  presents and explains many of the common variations, as well as many
  of the formatting elements an author may use in the preparation of
  the documentation of their work.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{keywords}
  LaTeX class \sep
  paper template \sep
  paper formatting \sep
  CEUR-WS
\end{keywords}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

In the field of Digital Humanities, \emph{Distant Reading} tools \cite{Moretti2014} allow researchers to quickly gain knowledge on textual corpora without actually reading them. Purposes of these methods are various, but can be mainly categorized into two groups: in the first case, these methods are used in order to tag, classify, or summarize large quantities of documents, in order to quickly structure information or to deliver a speech over the whole studied corpus \cite{Underwood2017}. Methods in this case rely heavily on Big Data and make an extensive use of Machine Learning, often with the help of supervised methods. In the second case, researchers use computational methods to underline hidden structures in a small corpus or even a single document, which helps them to refine their understanding of this corpus or to validate hypotheses \cite{Eve2017}. Methods in this setting can also rely on Machine Learning, but must typically be build with more caution and attention to details: corpora are smaller, analyses are closer to the work, and methods must transparent in order to appropriately interpret results. The use of exploratory tools and unsupervised methods is also preferred in this context, as it is less desirable to base methods on information coming from large external corpora. The proposed method in this article typically belongs to the second group, as it is unsupervised and can be applied on a single document.

When a single (or a few) \emph{literary work} is analyzed, a common practice is to study \emph{narrative entities} (characters, event, location, etc.) used by the author in her/his book \cite{Schmid2010}. Researchers are frequently interested in depicting them and in seeing how they interact with each other in the story. Various computational tools can help them in this tasks, to name a few, Named Entity Recognition tools \cite{Agarwal2013, Chaturvedi2017, Li2022}, Automatic Character Networks Extraction \cite{Labatut2019}, Sentiment Analysis and Topic Modeling \cite{Min2019}, Textometry \cite{Novakova2019}, and Word Embeddings \cite{Grayson2016, Heuser2017, Kerr2017}. All these methods have been used in order to explicitly show hidden structures constructed by the author in her/his work. It permits to find regularities and patterns, and can help to categorize particular narrative constructions, writing styles, or genres. These kind of methods can be great complement to classical analyses of literary works as they allow to efficiently summarized information which is otherwise quite diffuse.

In this article, we propose a general framework in order to automatically characterize various narrative entities in a literary work. The entire framework is exposed starting from a wide perspective, which is how to organize the textual data, and is narrowed down to a specific use, the study of character relationships in \emph{Les Misérables}, by Victor Hugo. Along this presentation, various choices are made to highlight a particular usage of this framework, but theses choices should be viewed as suggestion rather than rules: the real strength of this framework is its flexibility and the direction taken in this article is oriented for a defined task. To be more specific, we will show how to use \emph{embeddings} in order to locate \emph{character relationships} alongside the vocabulary. An association measure can then be constructed between these words and the relationships, which can help a practitioner to depict the latters. Four variations of this method are proposed, and are tested on \emph{Les Misérables}.

The idea behind this framework comes from the field of automatic extraction and analysis of \emph{character networks} from literacy works (see \cite{Labatut2019} for a survey). When building character networks from a textual narrative, one of the most widespread methods consists in dividing the studied work into $n$ \emph{narrative units} or \emph{contexts} $u_1, \ldots, u_n$, which can be, e.g., sentences, paragraphs, or chapters, and then counting the number of units where characters co-occurred \cite{Elsner2012, Lee2012, Rochat2014, Grener2017, Min2019}. Usually, the text constituting these units is discarded and the resulting network displays edges which roughly represent an aggregated number of interactions between characters. However, by doing so, the aggregation occurs on various type of interactions and gives little information about the type of relationship which exist between characters. Various improvements where proposed in order to weight \cite{Sack2014} or sign \cite{Krishnan2015} (or both \cite{Min2019}) the edges in the character networks. A particular inspiration for the current work is the article by Min and Park (2019) \cite{Min2019}, where authors also analyzed characters in \emph{Les Misérables} by building various signed and weighted networks, with the help of Sentiment Analysis and Topic Modeling. The current framework was build by expanding this idea of refining character relationships by formalizing the data structure and keeping directions of exploration as wide as possible. Embeddings \cite{Incitti2023} appeared to us to be the proper tool for achieving this. As a matter of fact, with embeddings, the textual contents of units are transformed into workable mathematical objects (the vectors), usable for various tasks, while conserving a maximum of information. The framework has been further generalized in order to be applicable on various type of narrative entities, but the presented case remains the study of character relationships in \emph{Les Misérables}. 

The current article is structured as followed. Section \ref{methodology} define the framework, with section \ref{data} defining the data organization, section  \ref{unit_embeddings} describing how to embed textual units, and section \ref{entity_embeddings} deriving entity vectors lying in the same space as units. In section \ref{case_studie}, we present the specific methodology and results for the case study of character relationships in \emph{Les Misérables}, and section \ref{conclusion} draw conclusions and perspectives about this work. All (Python) scripts and datasets used in this article, as well as extended results, can be found in the dedicated GitHub repository.\footnote{\url{https://github.com/gguex/char2char_vectors}.}

\section{Framework}
\label{methodology}

\subsection{Data organization}
\label{data}

In this article, a textual narrative is divided in $n$ \emph{textual units} $u_1, \ldots, u_n$, and is represented through two tables. The first one is well known in the field of textual analysis, and consists in the $(n \times v)$ \emph{unit-word contingency table} $\mathbf{N}$, as represented by Table 1, where $v$ is the vocabulary size. In this table, each row represents an unit, each column a word, and cells $n_{ij}$ counts the number of times the word $i$ appears in the unit. Using this table typically denotes a \emph{Bag-of-Words} approach in our analyses.

\begin{table}[h]
	\scriptsize
	\begin{tabular}{|c||c|c|c|c|c|c|}
		\hline
		& aller & allumer & apercevoir & bas & bon & ... \\
		\hline
		\hline 
		$u_{101}$ & 23 & 2 & 6 & 11 & 6 & ... \\
		\hline
		$u_{102}$ & 12 & 1 & 0 & 3 & 9 & ... \\
		\hline
		$u_{103}$ & 10 & 0 & 5 & 1 & 5 & ... \\
		\hline
		$u_{104}$ & 0 & 0 & 1 & 0 & 0 & ... \\
		\hline
	\end{tabular}
	\label{cont_table}
	\caption{A snippet of the unit-word contingency table $\mathbf{N}$ extracted from \emph{les Misérables}. Rows are chapters, columns are words in the vocabulary, and cell $n_{ij}$ counts the number of time word $j$ appear in chapter $i$.}
\end{table}

The second table is the \emph{unit-entity table}, noted $\mathbf{E}$. It has a size of $(n \times p)$ where $p$ is the number of \emph{narrative entities} found in the text and cells $e_{ij}$ indicates the presence, or the count for a weighted version, of entity $j$ in unit $i$. An narrative entity, in the context of this article, can be loosely defined in order to be flexible for various types of texts or analyses. It can roughly be seen as a recurring object with some importance in the narration. For example, it can be a location, an object, a character, a pair of characters (or even a triplet, a quadruplet, etc.), an oriented character interaction (e.g. a dialog), or even a particular recurring event containing multiple characters (e.g. a meeting). In this article, we mostly consider character and pairs of characters as entities, as shown in Table 2. Note that in the present case, we consider that a character or a pair of characters are present in the unit if character names (or aliases) are detected above a fixed threshold. A weighted version of this table, where $e_{ij}$ contains the number of occurrences of the entity $j$ in the unit $i$, is also possible. However, equations presented in this article are written for the presence/absence version.

\begin{table}[h]
	\scriptsize
	\begin{tabular}{|c||c|c|c|c|}
		\hline
		& Cosette & Thénardier & Valjean & ... \\
		\hline
		\hline 
		$u_{101}$ & 1 & 1 & 0 & ... \\
		\hline
		$u_{102}$ & 1 & 1 & 0 & ... \\
		\hline
		$u_{103}$ & 1 & 1 & 0 & ... \\
		\hline
		$u_{104}$ & 1 & 0 & 1 & ... \\
		\hline
	\end{tabular}
	\begin{tabular}{|c||c|c|c|c|}
		\hline
		& ... & Cosette-Thénardier & Cosette-Valjean & ... \\
		\hline
		\hline 
		$u_{101}$  	& ... & 1 & 0 &  ... \\
		\hline
		$u_{102}$ 	& ... & 1 & 0 & ... \\
		\hline
		$u_{103}$  	& ... & 1 & 0 & ... \\
		\hline
		$u_{104}$ 	& ... & 0 & 1 & ... \\
		\hline
	\end{tabular}
	\label{occ_table}
	\caption{A snippet of the unit-entity table $\mathbf{E}$ extracted from \emph{les Misérables}. Rows are chapters, columns are characters (top) and character pairs (bottom), and cell $e_{ij}$ denotes if $j$ appear in chapter $i$.}
\end{table}

This data organization already gives an orientation to subsequent analyses, and should be kept in mind of the practitioner. Textual unit are now considered as \emph{individuals} (in the statistical terminology), defined by their \emph{variables} contained in the different columns of both tables. Moreover, subsequent analyses are oriented in searching how the unit-entity table $\mathbf{E}$ has an influence over the unit-word table $\mathbf{N}$, i.e. searching which words are over-represented or under-represented knowing entities in the unit. While an authors uses characters in order to build her/his narrative, we, to a certain extent, work backward: we are searching how character appearances and interactions in the textual unit act on her/his choice of words. If the extraction method permits it, a practitioner should include all entities which she/he desires to study. Here, for example, the choice to include character pairs along with characters is motivated by the fact that we are interested in studying character relationships. A character pair can roughly be seen as an interaction between two characters, and this interaction should be considered as an object of its own: the presence of this interaction in a unit do not result in having a mixture of words used for each character, but rather gives a specific flavor to the unit. 

This data organization also highlights the importance of choosing a proper size for the units. These units should be large enough to contain enough words in order the properly capture the textual specificity of each unit, but not too large, as each unit should ideally captures particularities about one of the entity. Unfortunately, it is impossible to define an ideal size for all types of analysis and this size should be balanced regarding the level of analysis, the text size, the selected entities, and previous knowledge of the studied work.

The use of a contingency table $\mathbf{N}$ to represent the textual resource present in the units denotes a \emph{Bag-of-Words} approach. Using this approach loses the information relative to the order of words in the units, but permits to transform a chain of characters, improper to statistical analyses, into a contingency table, a well studied mathematical object which allows the use of various kind of computational methods. The next section shows a particular direction on how to use this table, with the help of embeddings.

\subsection{Embedding of textual units}
\label{unit_embeddings}

Various methods can be performed on the contingency table $\mathbf{N}$ in order to extract information from it. Here, we make to choice to extract a lower dimensional, numeric representation of each unit, in other words, a \emph{textual unit vector} located in an \emph{embedding space}. \\
In section \ref{entity_embeddings}, these vectors of textual units are used as anchor points in order to also embed entities into the same space. Therefore, it is crucial that an interpretation about the directions or the regions of this embedding space is possible, in order to properly interpret the localization of entity vectors (the relative position of entity vectors among themselves is generally insufficient). For that reason, we focus on embeddings of textual units which also contain \emph{vectors of words}: by examining the positions of entities relatively to word vectors, entities can be depicted. We propose two embeddings verifying this condition: Section \ref{ca_method} describes \emph{Correspondence Analysis (\textbf{CA})} and section \ref{wv_method} focuses on \emph{Pre-trained Word Vectors (\textbf{WV})}. 

\subsubsection{Correspondence Analysis (CA)}
\label{ca_method}

Using \emph{Correspondence Analysis} (\textbf{CA}) in order to analyze textual resources has a long tradition \cite{Lebart2019}. It has the advantage to naturally provide an embedding space, the factorial map, where units are placed alongside word vectors, and allows the interpretation of the placement of units in terms of word frequency profiles. Units and words vectors in the embedding space have a direct interpretation in terms of chi2 distance between profiles.

By performing a Correspondence Analysis on table $\mathbf{N}$, we get $n$ vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$ corresponding to units (rows) and $v$ vectors $\mathbf{w}_1, \ldots, \mathbf{w}_v$ corresponding to words (columns). Each of these vectors has a size of $\min(n, v) - 1$, which will generally be $n - 1$. For a detailed computation of quantities in CA, see Appendix \ref{ca_details}. 

An \emph{association score} between a particular unit $i$ and a word $j$ is expressed through the scalar product between their vectors 
\begin{equation}
a_{ij} := \mathbf{x}^\top_i \mathbf{w}_j.
\end{equation}
A positive (resp. negative) association score denotes a over-representation (resp. under-representation) of the word $j$ in $i$, which permit to find lists of words characterizing the different units. Observe that in this article, this association score is rather computed between a word vector and a entity vector, since the latter, as we will see in section \ref{entity_embeddings}, lies in the same space as unit vectors. We could also track how units (or entities) are dissimilar to each others by using this time the Euclidean distance between vectors.

Note that vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$ obtained from \textbf{CA} reflect textual unit profile (in terms of words) regarding the mean profile (the origin in the factorial map). This analysis is thus \emph{contrastive}: it highlights unit variations among the studied text. It means that the particular tone of the whole studied text might be hidden in this analysis and only the variation around this tone will be revealed. It might lead to the situation where the (absolute) feeling experienced by the reader will not appear in this analysis, e.g., a sad character in a sad book might appear joyful if he is less sad than the mean tone. This can become problematic when this method is used sequentially to study multiple works: particularities of each book will be hidden. Another limitation with this approach is that the words helping the interpretation of units (and entities) are contained in the studied text. Approaches requiring to study the position of units and entities relatively to a predefined list of words (e.g., friends, enemies, family) might therefore be impossible if these words do not appear in the text.

\subsubsection{Pre-trained Word Vectors (WV)}
\label{wv_method}

\emph{Pre-trained Word Vectors}$  $ (\textbf{WV}), based on methods such as Word2Vec \cite{Mikolov2013}, GloVe \cite{Pennington2014}, fastText \cite{Bojanowski2017}, or Bert \cite{Devlin2018} have received great attention from various fields in the last decade. They are generally obtained through a training on a very large corpora, such as Wikipedia or Common Crawl, and the resulting embedding contains a large quantity of word vectors. As shown by multiple studies (see \cite{Li2017} for a survey), these vectors are placed in order to reflect semantic and syntactic relationships between words, and are used in various applications. We focus here on \emph{static} word embeddings, where word vectors are fixed and do not depends on their context, obtained by, e.g., fastText. The reason is that we need to have interpretable regions in an unchanging embedding space.

Multiple methods exist which use pre-trained word vectors in order to derive vectors for a \emph{group of words}, such as sentences \cite{Grener2017, Arora2017}, paragraphs \cite{Le2014}, or documents \cite{Kusner2015}. These derived vectors are often used to apply a classification or clustering algorithm on the newly embedded objects, or to query information \cite{Kusner2015, Arora2017}. In order to derive these vectors, the majority of methods use frequencies of words found in objects, i.e. a table similar to $\mathbf{N}$, but apply various weighting scheme and normalization in order to reduce the effects of frequent words and to standardize vectors. In the present article, we use a methodology proposed in \cite{Arora2017} as it is compatible with multiple unit sizes and gives good results in many tasks. Thus, textual units vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$ are obtained through the table $\mathbf{N}$ and with the method detailed in Appendix \ref{wv_details}. 

An \emph{association score} can again be computed between an unit (or an entity) vector $\mathbf{x}_i$ and word vector $\mathbf{w}_j$ through the cosine similarity, defined by 
\begin{equation}
a_{ij} := \frac{\mathbf{x}_i^\top \mathbf{w}_j}{\sqrt{\mathbf{x}_i^\top \mathbf{x}_i \mathbf{w}_j^\top \mathbf{w}_j}}
\end{equation}
Note that, with word vectors, this cosine similarity also permits to compare units (or entities) between themselves.

With the pre-trained word vector method, the unit vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$ (and entity vectors in section \ref{entity_embeddings}) lie in an \emph{absolute space} defined by the pre-trained word vectors. Comparison between different texts are therefore more pertinent and associations with words absent from the corpus can be made. However, it is possible that all units from a given text will be located in the same region of the space if the vocabulary used in it is very specific. In this case, the list of most associated word vectors might be similar for every unit, and the analysis will not give satisfying results. This effect is fortunately limited by the centration of unit vectors which occurs in the method described in Appendix \ref{wv_details}.

\subsection{Entity embeddings}
\label{entity_embeddings}

The main goal of this article is not to analyze units, but rather entities, i.e., the $p$ columns of table $\mathbf{E}$. While we use the table $\mathbf{N}$ to build embeddings of units, we utilize the table $\mathbf{E}$ in order to build the entity vectors $\mathbf{y}_1, \ldots, \mathbf{y}_p$ relatively to unit vectors $\mathbf{x}_1, \ldots, \mathbf{x}_n$. Two propositions of methods are made: the \emph{centroids method} (\textbf{CENT}), described in section \ref{centroid}; and the \emph{regressions method} (\textbf{REG}), explained in section \ref{regression}. Both methods can be combined with the embeddings of units defined in the previous section.

\subsubsection{Centroids (CENT)}
\label{centroid}

This method is the most trivial and is based on the following intuition: an entity is characterized equally by all units in which it appears. In other words, we can define the vector $\mathbf{y}_k$ for entity $k$ as
\begin{equation}
\mathbf{y}_k = \sum_{i=1}^n f_i e_{ik} \mathbf{x}_i
\end{equation}
where $f_i = \frac{n_{i \bullet}}{n_{\bullet \bullet}}$ is the relative weight of unit $i$. $\mathbf{y}_k$ indicates the center of mass, or \emph{centroid}, of the units containing the entity. This way of building entity vectors is closely related to the treatment of \emph{supplementary variables} found in \textbf{CA}: these variables do not act in the choice of factorial axis, but can still be represented afterward. However, by contrast, entity vectors are not dilated after computing centroids, which means that they lie in the same space as units (row). \\
An important remark about the centroid method, is that entity vectors positions are \emph{additive}, i.e. we have
\begin{equation}
e_{ik} = \sum_{g \in \mathcal{G}} e_{ig}, \forall i \implies \mathbf{y}_k = \sum_{g \in \mathcal{G}} \mathbf{y}_g,
\end{equation}
where $\mathcal{G}$ is a subset of entities. This property can be interpreted as followed: if a character $k$ can be divided among different situations $g$ (the character alone, the character in interaction with another character, etc.), the character vector $\mathbf{y}_k$ is in fact the sum of all vectors $\mathbf{y}_g$ of these situations. This is not necessary an undesirable property, but it implies that the specificities of the lone character might be hidden if he is often registered in an interaction. By contrast, if we consider that an interaction between two characters is an \emph{emerging situation}, unrelated to prior behaviors of characters, the regressions method described in the next section seems more appropriate.

\subsubsection{Regressions (REG)}
\label{regression}

When building a regression model with multiple explanatory variables, it is possible to include every variables and their \emph{interactions}. By doing so, we suppose that the effect of raising both variables is not the same as raising each variable independently. Regression models seem therefore appropriate to capture specificities of having a particular entity in a textual unit. For example, in the case of character pairs, the presence of a character $a$ will have a effect on the vocabulary of an unit, the presence of another character $b$ will have another effect, and the presence of the pair $\{a, b\}$ yet a different effect. Now, dependent variables in regression models still need to be defined. In fact, we are doing $d$ regressions, with $d$ the number of dimensions of the embedding, and each regression is constructed to predict the $\alpha$-th coordinate of units by using binary variables in the table $\mathbf{E}$. In matrix notation, all regression models can be written as
\begin{align}
\mathbf{X} = \widetilde{\mathbf{E}} \mathbf{B} + \bm{\Sigma}, \label{cent_sol}
\end{align}
where $\mathbf{X} = (x_{i\alpha})$ is the $(n \times d)$ matrix containing unit vectors (on rows), $\widetilde{\mathbf{E}}$ is the matrix $\mathbf{E}$ with an first additional column of $1$ for the intercept, $\mathbf{B} = (\beta_{k\alpha})$ is the $((p+1) \times d)$ matrix containing intercepts and regression coefficients (each column corresponds to one regression), and $\mathbf{\Sigma}$ the $(n \times \alpha)$ matrix containing normal errors. 

Intercepts and coefficients estimations $\widehat{\mathbf{B}} = (\widehat{\beta}_{k\alpha})$ can be considered as our embeddings for entities as well as for the intercept, which represents the general tone of the studied text. We therefore denote these estimates with $\mathbf{Y} = (y_{k\alpha})$ in the following, with the notation convention $y_{0\alpha}$ for intercept coordinate $\alpha$. 

As the number of entities (i.e. predictors) might be very large, it is a good idea to add a $L^2$ regularization term in the objective function. Moreover, the quadratic error rate should also be weighted by the number of tokens in each unit. Including all this, we find the solution for our intercept and entity vectors $\mathbf{y}_0, \mathbf{y}_1, \ldots, \mathbf{y}_p$, contained in the rows of $\mathbf{Y}$, with 
\begin{equation}
\mathbf{Y} = (\widetilde{\mathbf{E}}^\top \textbf{Diag}(\mathbf{f}) \widetilde{\mathbf{E}} + \lambda \mathbf{I}_{(p+1)})^{-1} \widetilde{\mathbf{E}}^\top \textbf{Diag}(\mathbf{f}) \mathbf{X}, \label{reg_sol}
\end{equation}
where $\textbf{Diag}(\mathbf{f})$ is the diagonal matrix containing weights of units $\mathbf{f} = \left( f_i \right)$, $\lambda > 1$ is the regularization coefficient, and $\mathbf{I}_{(p+1)}$ is the identity matrix of size $((p+1) \times (p+1))$.

An interesting effect of the regularization coefficient is that if $\lambda$ is high, equation (\ref{reg_sol}) becomes $\mathbf{Y} \approx \frac{1}{\lambda} \widetilde{\mathbf{E}}^\top \textbf{Diag}(\mathbf{f}) \mathbf{X}$, which is similar to equation (\ref{cent_sol}) with a contraction term $\lambda$. In fact, the regressions method with a regularized term interpolate between the hypothesis where we suppose that every entity should be considered independently (with $\lambda \to 0$), to the hypothesis of additive mixture between entities (with $\lambda \to \infty$), as discussed in section \ref{centroid}. Choosing an appropriate $\lambda$ according to the study (how is another, difficult question) might lead to a situation revealing desirable information about entities.

\section{Case studie : \emph{Les Misérables}}
\label{case_studie}

At the time of writing, it is not possible to evaluate the exposed framework with some kind of metric, which would allow to test its pertinence on various corpora. In order to see if the methods give coherent results, we have to carefully scrutinized and compared them with previous knowledge of the studied work. For this reason, and because of method variations and multiplicity of the results (and lack of place), we chose to present only one case study: the analysis of characters and relationships in \emph{Les Misérables}, by Victor Hugo. The choice of this work is motivated by the fact that it is a large corpus, well-known, immensely studied, and containing various colorful characters and characters relationships. It is therefore a solid choice to clearly illustrate the potential of the exposed framework.

\subsection{Preprocessing}

The five volumes of \emph{Les Misérables}, in French, were extracted from \emph{Project Gutenberg}\footnote{\url{https://www.gutenberg.org/}.}, while headers and footers of each files were manually removed. The whole text was lower cased, lemmatized, and stopwords\footnote{from a list made by Jacques Savoy \url{http://members.unine.ch/jacques.savoy/clef/frenchST.txt}.} and punctuation were removed. Volumes, books, and chapters breaking points were kept for later uses. 

We chose to use chapters as textual units. The table $\mathbf{N}$ (Figure \ref{cont_table}) was build by considering words appearing at least $20$ times in the text and resulted in a table of size $365$ chapters $\times$ $1974$ words.

Characters were detected using \emph{Flair}\footnote{\url{https://github.com/flairNLP/flair}.} NER tools \cite{Akbik2018}. In order to unify characters and to further refine the results, we used hand-made lists of character names and aliases from NER results. It resulted in the detection of 54 characters. The entities considered in table $\mathbf{E}$ (Figure \ref{occ_table}) are composed of 54 single characters and 547 character pairs, resulting in a table of size $365 \times 601$. A character (resp. a pair of characters) is considered present if it is (resp. both are) detected at least 2 times in the chapter. 

Note that, in section \ref{diachronic}, we also tested experiments with entities consisting in characters and character pairs as found in each volumes (e.g. Cosette-Valjean in volume one and Cosette-Valjean in volume two are now two different entities), with the addition of volume constants ($V_i=1$ in volume $i$ and $V_i=0$ in other volumes) in order to isolate volume specific vocabulary. This new table $\mathbf{E}_\text{vol}$, containing $1124$ entities, permits to see a diachronic evolution of words associated with volumes, characters, and character relationships. 

\subsection{Methods}

There are two types of methods for unit embeddings, \textbf{CA} (section \ref{ca_method}) and \textbf{WV} (section \ref{wv_method}), as well as two methods to derive entity embeddings from them, \textbf{CENT} (section \ref{centroid}) and \textbf{REG} (section \ref{regression}), making a total of 4 possibles ways for obtaining entity embeddings. 

The \textbf{CA} method do not need any external data, and results in vectors in a $364$-dimensional space, while the $\textbf{WV}$ methods is based on pre-trained word vectors using \emph{fastText} \cite{Bojanowski2017} trained on Common Crawl.\footnote{\url{ https://fasttext.cc/docs/en/crawl-vectors.html}.} For French, the number of word vectors is 2 millions and the dimension of the vector space is $300$. 

Note that, in addition to having 2 tables $\mathbf{E}$ and $\mathbf{E}_\text{vol}$, 4 methods, and a considerable number of words and entities, results can also be presented in various ways (similarities between entities, associations between entities and words, etc.). Thus, we chose to show here a selection of results for the each method: the 5 most associated words regarding a subset of entities (section \ref{words}), the 5 most associated entities regarding a subset of words (section \ref{objects}), and a diachronic study of the 5 most associated words for a subset of entities (section \ref{diachronic}). We invite curious readers to consult results for all words and entities, which can be found in our GitHub repository.\footnote{in the "results" folder in \url{https://github.com/gguex/char2char_vectors}.}

\subsection{Results}

\subsubsection{The most associated words for a subset of entities}
\label{words}

%------------------------------

\begin{table*}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{\textbf{CA-CENT}}}} & Cosette & Cosette-Marius & Cosette-Valjean & Marius & Valjean \\ 
		\cline{2-6}
		& poupée (0.7) & \textbf{noce} (1.72) & \textbf{noce} (1.0) & théodule (0.61) & \textbf{mestienne} (0.51) \\
		& \textbf{noce} (0.68) & \textbf{mariage} (1.31) & \textbf{mestienne} (0.97) & jondrette (0.59) & fossoyeur (0.46) \\
		& \textbf{mestienne} (0.58) & \textbf{marié} (1.21) & \textbf{mariage} (0.71) & \textbf{ursule} (0.56) & \textbf{accusé} (0.45) \\
		& \textbf{mariage} (0.48) & marier (1.11) & \textbf{marié} (0.68) & vernon (0.53) & maire (0.39) \\
		& \textbf{marié} (0.48) & baron (1.0) & corbillard (0.65) & tante (0.52) & jean (0.37) \\ 
		\cline{2-6}
		& Marius-Valjean & Javert & Javert-Valjean & Myriel & Myriel-Valjean \\
		\cline{2-6}
		& \textbf{noce} (1.2) & \textbf{accusé} (1.47) & \textbf{accusé} (1.85) & conventionnel (5.03) & chandelier (6.28) \\
		& \textbf{mariage} (0.85) & arras (1.04) & \textbf{avocat} (1.12) & évêque (3.54) & gendarme (5.06) \\
		& \textbf{ursule} (0.85) & mouchard (0.97) & \textbf{preuve} (1.1) & oratoire (3.39) & panier (4.72) \\
		& \textbf{marié} (0.8) & \textbf{avocat} (0.96) & président (1.08) & hôpital (2.57) & couvert (4.64) \\
		& tableau (0.74) & \textbf{preuve} (0.93) & forçat (1.01) & cathédrale (2.54) & deuil (4.52) \\
		\hline
		\hline
		\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{\textbf{CA-REG}}}} & Cosette & Cosette-Marius & Cosette-Valjean & Marius & Valjean \\ 
		\cline{2-6}
		& seau (1.23) & amant (0.83) & blessure (0.78) & jondrette (1.76) & matelas (1.02) \\
		& poupée (0.86) & mariage (0.73) & \textbf{noce} (0.76) & réchaud (1.26) & \textbf{chandelier} (0.87) \\
		& ravissant (0.7) & entraîner (0.7) & file (0.6) & galetas (1.11) & toulon (0.82) \\
		& source (0.65) & \textbf{noce} (0.67) & corbillard (0.58) & bouge (1.05) & fossoyeur (0.79) \\
		& rassurer (0.61) & volupté (0.63) & mestienne (0.58) & tableau (0.93) & pelle (0.76) \\ 
		\cline{2-6}
		& Marius-Valjean & Javert & Javert-Valjean & Myriel & Myriel-Valjean \\
		\cline{2-6}
		& égout (1.1) & arras (1.09) & accusé (1.04) & conventionnel (2.99) & deuil (1.14) \\ 
		& vase (1.08) & roue (0.89) & nier (0.79) & évêque (1.76) & \textbf{chandelier} (1.07) \\ 
		& issue (1.07) & bonjour (0.83) & quai (0.54) & cathédrale (1.14) & aveugle (1.01) \\ 
		& sable (1.0) & malle (0.8) & avocat (0.53) & prêtre (1.11) & panier (0.94) \\
		& couloir (0.98) & cabriolet (0.76) & fonction (0.5) & philosophie (1.06) & gendarme (0.89) \\ 
		\hline
		\hline
		\hline
		\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{\textbf{WV-CENT}}}} & Cosette & Cosette-Marius & Cosette-Valjean & Marius & Valjean \\ 
		\cline{2-6}
		& \textbf{jean} (0.34) & aimer (0.38) & \textbf{jean} (0.6) & embrasser (0.36) & \textbf{jean} (0.56) \\
		& dormir (0.28) & rêver (0.34) & \textbf{jacques} (0.3) & \textbf{essayer} (0.36) & \textbf{habiller} (0.27) \\ 
		& regarder (0.26) & \textbf{vouloir} (0.32) & \textbf{philippe} (0.26) & \textbf{avouer} (0.36) & \textbf{poser} (0.26) \\
		& \textbf{habiller} (0.26) & douter (0.32) & \textbf{habiller} (0.26) & \textbf{vouloir} (0.35) & \textbf{jacques} (0.26) \\
		& \textbf{voir} (0.25) & \textbf{avouer} (0.32) & \textbf{pantalon} (0.25) & \textbf{voir} (0.35) & \textbf{pantalon} (0.25) \\
		\cline{2-6}
		& Marius-Valjean & Javert & Javert-Valjean & Myriel & Myriel-Valjean \\
		\cline{2-6}
		& \textbf{jean} (0.35) & \textbf{saisir} (0.34) & \textbf{jean} (0.54) & \textbf{évêque} (0.59) & \textbf{évêque} (0.55) \\
		& questionner (0.31) & \textbf{jean} (0.34) & denis (0.31) & \textbf{archevêque} (0.52) & \textbf{archevêque} (0.46) \\
		& \textbf{essayer} (0.31) & placer (0.31) & \textbf{jacques} (0.3) & \textbf{prêtre} (0.45) & \textbf{prêtre} (0.42) \\
		& oser (0.31) & retirer (0.29) & \textbf{saisir} (0.3) & \textbf{abbé} (0.39) & âme (0.42) \\
		& \textbf{poser} (0.29) & dégager (0.29) & \textbf{philippe} (0.28) & souverain (0.38) & \textbf{abbé} (0.39) \\
		\hline
		\hline
		\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{\textbf{WV-REG}}}} & Cosette & Cosette-Marius & Cosette-Valjean & Marius & Valjean \\ 
		\cline{2-6}
		& contempler (0.29) & éternel (0.35) & \textbf{rue} (0.44) & regarder (0.38) & \textbf{jean} (0.56) \\
		& emplir (0.29) & \textbf{amour} (0.35) & \textbf{jean} (0.41) & voir (0.36) & pantalon (0.28) \\
		& doucement (0.27) & humanité (0.34) & faubourg (0.41) & refermer (0.34) & jacques (0.26) \\
		& envelopper (0.26) & \textbf{âme} (0.32) & \textbf{boulevard} (0.41) & \textbf{glisser} (0.34) & philippe (0.23) \\
		& illuminer (0.26) & vérité (0.32) & quartier (0.34) & poser (0.31) & \textbf{glisser} (0.23) \\ 
		\cline{2-6}
		& Marius-Valjean & Javert & Javert-Valjean & Myriel & Myriel-Valjean \\
		\cline{2-6}
		& \textbf{rue} (0.35) & serrer (0.34) & \textbf{rue} (0.35) & \textbf{évêque} (0.43) & ange (0.37) \\
		& \textbf{boulevard} (0.35) & \textbf{glisser} (0.34) & \textbf{boulevard} (0.34) & divin (0.4) & \textbf{évêque} (0.31) \\
		& souterrain (0.35) & forcer (0.34) & autorité (0.33) & humble (0.39) & \textbf{âme} (0.31) \\
		& bastille (0.35) & bouger (0.33) & civil (0.33) & bonté (0.38) & \textbf{amour} (0.29) \\
		& carrefour (0.34) & aller (0.32) & loi (0.33) & archevêque (0.37) & aurore (0.28) \\
		\hline
	\end{tabular}
	
	\label{word_vs_obj}
	\caption{The 5 most associated words (association score in parentheses) to a selected set of entities, regarding \textbf{CA-CENT}, \textbf{CA-REG}, \textbf{WV-CENT}, and \textbf{WV-REG} methods. Words appearing at least two times within the same method are in bold.}
\end{table*}
	
%------------------------------

The first results in this section is to examine the most associated words with a subset of entities, as measured by the association score defined in section \ref{unit_embeddings}. Results can be found in Table 3 for all methods.

We can first observe that \textbf{CA} methods seems to summarize entities with a vocabulary closer to the work, while \textbf{WV} methods tend to frequently use words with a wider scope, with notably more verbs. It results in having the \textbf{WV} methods giving a general feeling for the tone used for describing characters and relationships, while the \textbf{CA} methods can depict very specific objects, locations or events associated with these entities. This behavior can be understood by the nature of unit embeddings: in the \textbf{WV} embedding, word vectors are fixed and do not take into account the actual frequencies of words found in the studied corpora. A character can be close to a word appearing only a few times (or none) in the corpus if this word is located near the vocabulary associated with this character, as semantically similar words are located in the same region of space. By contrast, \textbf{CA} will generally takes into account word frequencies along with specificities in order to describe an entity, and semantically similar words can be located far away from each others.

Another remark can be made about the difference between \textbf{CENT} methods and \textbf{REG} methods. As expected, we see that the \textbf{CENT} methods reveal their additive construction between characters and relationships: words used to describe a relationship rob off on their characters descriptions (see e.g. Cosette, Cosette-Marius, and Cosette-Valjean). By constrast, the \textbf{REG} methods display more "perpendicular" descriptions of entities, with less words repeating.   

Note that we did not show here the least associated words with each entities, as they are frequently the same for all methods and all related to the long description of the Battle of Waterloo in volume 2 ("infanterie", "wellington", "cuirassier", "bridage"), containing no protagonist of the story. 

Overall, we find that the \textbf{CA-REG} method gives the most satisfying results, with pertinent words associated with each entity, and an high variety in the choice of words. 
	
\subsubsection{The most associated entities for a subset of words}
\label{objects}

%------------------------------

\begin{table*}[!bh]
	\centering
	\scalebox{0.82}{	
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		& aimer & rue & justice & guerre \\ \hline
		\hline
		\parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{CA-CENT}}}} & Dahlia-Fameuil (1.12) & Courfeyrac-Fauchelevent (0.79) & Azelma-Babet (1.09) & Combeferre-Fauchelevent (1.35) \\
		& Dahlia-Listolier (1.12) & Courfeyrac-Toussaint (0.79) & Azelma-Brujon (1.09) & Feuilly-Valjean (1.27) \\
		& Fameuil-Zéphine (1.12) & Eponine-Fauchelevent (0.79) & Azelma-Claquesous (1.09) & Feuilly-Marius (1.22) \\
		& Listolier-Zéphine (1.12) & Eponine-Gavroche (0.79) & Azelma-Magnon (1.09) & Lesgle-Valjean (1.15) \\
		& Gillenormand-Toussaint (1.11) & Eponine-Pontmercy (0.79) & Azelma-Montparnasse (1.09) & Mabeuf-Valjean (1.15) \\ 
		\hline
		\hline
		\parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{CA-REG}}}} & Cosette-Marius (0.35) & Courfeyrac (0.37) & Javert-Valjean (0.37) & Grantaire-Pontmercy (0.94) \\
		& Myriel (0.31) & Grantaire-Prouvaire (0.29) & Champmathieu-Valjean (0.37) & Grantaire (0.56) \\
		& Basque-Fauchelevent (0.25) & Cosette-Javert (0.26) & Myriel (0.21) & Marius-Pontmercy (0.55) \\
		& Myriel-Valjean (0.22) & Marius-Prouvaire (0.22) & Grantaire (0.18) & Pontmercy (0.55) \\
		& Fauchelevent-Gillenormand (0.21) & Enjolras (0.22) & Grantaire-Javert (0.16) & Enjolras (0.49) \\ 
		\hline
		\hline
		\hline
		\parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{WV-CENT}}}} & Cosette-Marius (0.38) & Grantaire-Prouvaire (0.57) & Azelma-Babet (0.34) & Grantaire (0.32) \\
		& Fantine-Marius (0.34) & Marius-Prouvaire (0.54) & Azelma-Brujon (0.34) & Combeferre-Lesgle (0.32) \\
		& Fantine-Pontmercy (0.34) & Cosette-Javert (0.43) & Azelma-Claquesous (0.34) & Feuilly-Lesgle (0.25) \\
		& Basque-Fauchelevent (0.34) & Magnon-Monsieur Thénardier (0.37) & Azelma-Magnon (0.34) & Combeferre-Marius (0.25) \\
		& Prouvaire-Valjean (0.33) & Gavroche (0.36) & Azelma-Montparnasse (0.34) & Combeferre-Grantaire (0.25) \\ 
		\hline
		\hline
		\parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{WV-REG}}}} & Prouvaire-Valjean (0.34) & Grantaire-Prouvaire (0.8) & Champmathieu-Valjean (0.38) & Grantaire-Pontmercy (0.39) \\
		& Champmathieu-Chenildieu (0.31) & Marius-Prouvaire (0.77) & Azelma-Brujon (0.35) & Enjolras-Marius (0.35) \\
		& Brevet-Chenildieu (0.31) & Courfeyrac (0.66) & Azelma-Claquesous (0.35) & Grantaire (0.31) \\
		& Brevet-Cochepaille (0.31) & Cosette-Javert (0.64) & Azelma-Magnon (0.35) & Combeferre-Lesgle (0.31) \\
		& Champmathieu-Cochepaille (0.31) & Prouvaire (0.63) & Azelma-Montparnasse (0.35) & Cosette-Gavroche (0.27) \\ \hline
	\end{tabular}
	}
	\label{CA_CENT_obj_words}
	\caption{The 5 most associated entities (association score in parentheses) to a selected set of words, regarding \textbf{CA-CENT}, \textbf{CA-REG}, \textbf{WV-CENT}, and \textbf{WV-REG} methods.}
\end{table*}

%------------------------------

%------------------------------

\begin{figure*}[!bh]
	\centering
	\includegraphics[width=0.45\textwidth]{fig/aimer.png} \includegraphics[width=0.45\textwidth]{fig/rue.png}
	\includegraphics[width=0.45\textwidth]{fig/justice.png}
	\includegraphics[width=0.45\textwidth]{fig/guerre.png}
	\label{networks}
	\caption{Resulting weighted and signed networks between main characters, with examples of word queries ("aimer", "rue", "justice", and "guerre"). These networks are computed with \textbf{CA-REG} method ($\lambda = 0.01$). Red indicate positive affinity, blue negative affinity, and edge width is proportional to the number of detected interactions between characters.}
\end{figure*}

%------------------------------

These results are extracted from a transposed table, and display the most associated entities to a selected set of words. They can be found in Table 4. This type of results can be seen as queries, made from a single word by a practitioner, which output the most associated entities in the work related to that query. We chose here to show top entities related to words "aimer", "rue", "justice" and "guerre", as they represent some of the main topics of the book. In this task again, from our point of view, the \textbf{CA-REG} displays the most accurate results: the main love relationship (Cosette-Marius) of the book is the most associated entity for "aimer", several "amis de l'ABC" (a revolutionary group) are most associated with "rue", the cop-suspect relationship (Javert-Valjean) is the top entity for "justice", and military officers or bellicose characters are associated with "guerre". While somewhat inferior with the selected set of queries, \textbf{WV} methods have the advantage to be able to query words outside the scope of the book, as the pre-trained word embedding possess a very large vocabulary.

Note that another way to represent these results is through weighted signed networks, as found in Figure 1 (for \textbf{CA-REG}). The network structure represent the number of times characters are detected together (which do not depend on the query), and the signed weights (edge color) display association score between character relationship (edges) and the queried word. This representation gives a quick visual support in order to explore the studied work and could be implemented as a standalone program.


\subsubsection{A diachronic study of the most associated words for a subset of entities}
\label{diachronic}

\begin{table*}[!h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\parbox[t]{2mm}{\multirow{24}{*}{\rotatebox[origin=c]{90}{\textbf{CA-REG}}}} 
		& $V_1$ & $V_2$ & $V_3$ & $V_4$ & $V_5$ \\ 
		\cline{2-6}
		& huissier (1.4) & cuirassier (3.38) & gamin (1.92) & émeute (1.48) & sable (3.04) \\
		& hôte (1.11) & infanterie (2.9) & mine (1.38) & révolte (0.86) & berge (2.32) \\
		& arras (0.92) & sacrement (2.69) & farce (0.81) & bourgeoisie (0.84) & égout (2.16) \\
		& lampe (0.91) & brigade (2.41) & ignorance (0.74) & populaire (0.82) & voûte (1.9) \\
		& montreuil (0.81) & division (2.4) & jondrette (0.7) & insurrection (0.81) & vase (1.89) \\ 
		\cline{2-6}
		& Valjean 1 & Valjean 2 & Valjean 3 & Valjean 4 & Valjean 5 \\ 
		\cline{2-6}
		& chandelier (1.03) & pelle (2.6) & ursule (1.49) & réverbère (0.98) & matelas (2.54) \\ 
		& toulon (0.8) & fossoyeur (2.35) & luxembourg (1.06) & hausser (0.45) & ronde (1.96) \\
		& gervai (0.73) & pioche (1.51) & tableau (0.93) & promenade (0.37) & galerie (1.06) \\ 
		& bagne (0.65) & carte (1.39) & banc (0.81) & lanterne (0.36) & lanterne (0.99) \\ 
		& maire (0.57) & mestienne (1.35) & mouchoir (0.77) & tuyau (0.35) & rive (0.99) \\ 
		\cline{2-6}
		& Cosette 1 & Cosette 2 & Cosette 3 & Cosette 4 & Cosette 5 \\ 
		\cline{2-6}
		& gargote (0.59) & seau (1.48) & - & ravissant (1.11) & encre (0.76) \\ 
		& balayer (0.57) & poupée (0.99) & - & céleste (0.76) & plume (0.59) \\
		& alouette (0.56) & source (0.84) & - & volupté (0.67) & noce (0.49) \\
		& servante (0.43) & gargote (0.71) & - & frémir (0.64) & chandelier (0.48) \\
		& mois (0.34) & mestienne (0.64) & - & lancier (0.6) & antichambre (0.47) \\
		\cline{2-6}
		& Cosette-Valjean 1 & Cosette-Valjean 2 & Cosette-Valjean 3 & Cosette-Valjean 4 & Cosette-Valjean 5 \\
		\cline{2-6}
		& maladie (0.49) & façade (0.68) & - & promenade (0.5) & noce (1.27) \\ 
		& médecin (0.48) & corbillard (0.66) & - & chaîne (0.47) & marié (0.93) \\
		& demain (0.33) & mestienne (0.6) & - & blessure (0.46) & mardi (0.89) \\
		& surprise (0.28) & bâtiment (0.56) & - & tuyau (0.45) & mariage (0.86) \\
		& auprès (0.28) & cul (0.55) &- & luxembourg (0.44) & file (0.65) \\ 
		\hline
		\hline
		\parbox[t]{2mm}{\multirow{24}{*}{\rotatebox[origin=c]{90}{\textbf{WV-REG}}}} 
		& $V_1$ & $V_2$ & $V_3$ & $V_4$ & $V_5$ \\ 
		\cline{2-6}
		& demander (0.36) & saint (0.39) & gamin (0.45) & violence (0.44) & égout (0.52) \\ 
		& décider (0.3) & mont (0.39) & garçon (0.42) & haine (0.42) & quai (0.45) \\ 
		& aider (0.3) & régiment (0.38) & jeune (0.36) & révolte (0.42) & rue (0.44) \\ 
		& expliquer (0.29) & chapelle (0.38) & enfant (0.35) & souffrance (0.4) & eau (0.42) \\
		& plaindre (0.29) & infanterie (0.36) & père (0.34) & étincelle (0.39) & chaussée (0.42) \\
		\cline{2-6}
		& Valjean 1 & Valjean 2 & Valjean 3 & Valjean 4 & Valjean 5 \\ 
		\cline{2-6}
		& essayer (0.29) & jean (0.54) & admirer (0.32) & jean (0.71) & jean (0.72) \\ 
		& réfléchir (0.27) & jacques (0.33) & passer (0.31) & jacques (0.41) & pantalon (0.42) \\ 
		& expliquer (0.24) & pantalon (0.29) & observer (0.3) & pantalon (0.36) & jacques (0.39) \\ 
		& agir (0.24) & mr (0.28) & guetter (0.3) & louis (0.34) & philippe (0.34) \\ 
		& questionner (0.24) & denis (0.28) & croiser (0.3) & philippe (0.33) & denis (0.33) \\ 
		\cline{2-6}
		& Cosette 1 & Cosette 2 & Cosette 3 & Cosette 4 & Cosette 5 \\ 
		\cline{2-6}
		& an (0.41) & dormir (0.33) & - & rêver (0.32) & rêver (0.31) \\
		& mois (0.39) & regarder (0.29) & - & regarder (0.31) & mentir (0.29) \\
		& mère (0.36) & sentir (0.28) & - & contempler (0.28) & écrire (0.29) \\
		& fille (0.35) & endormir (0.28) & - & pleurer (0.27) & demander (0.28) \\
		& enfant (0.33) & respirer (0.28) & - & lire (0.27) & pleurer (0.28) \\
		\cline{2-6}
		& Cosette-Valjean 1 & Cosette-Valjean 2 & Cosette-Valjean 3 & Cosette-Valjean 4 & Cosette-Valjean 5 \\
		\cline{2-6}
		& voir (0.32) & rue (0.55) & - & jean (0.52) & mariage (0.42) \\ 
		& entendre (0.31) & ruelle (0.48) & - & pantalon (0.35) & marié (0.4) \\ 
		& frissonner (0.3) & boulevard (0.45) & - & gilet (0.28) & noce (0.4) \\ 
		& grommeler (0.3) & mur (0.4) & - & gris (0.27) & gai (0.33) \\ 
		& essayer (0.3) & faubourg (0.4) & - & manteau (0.26) & amour (0.3) \\ 
		\hline
	\end{tabular}
	\label{TIME_REG_word_vs_obj}
	\caption{The 5 most associated words (association score in parentheses) vs volumes constant, Valjen, Cosette, and Cosette-Valjean, as found in each tome regarding the \textbf{CA-REG} and \textbf{WV-REG} methods ($\lambda = 0.01$).}
\end{table*}

These results are obtained from the table $\mathbf{E}_\text{vol}$ where entities are considered different based on the volume. By doing so, it permits to track the evolution of association scores along the book. Additionally to entities, we can also define a constant term $V_i$ for each volumes $i$, which absorbs the associated words with each volume. Results for constants and a subset of entities (Valjean, Cosette, Cosette-Valjean) can be found in Table 5. Note that we did not show \textbf{CENT} results in this table, as they are similar to the one found in Table 3: words are often repeated for different entities and are less convincing. 

Here again, we see that associated words for the \textbf{WV} give the general tone of tomes and entities, while \textbf{CA} results are more specific and related to particular events which occurred for characters. As expected, words associated with volume constants give a short overview of each volumes, especially with the \textbf{CA-REG} method (e.g. $V_2$ for the Battle of Waterloo, $V_4$ for the barricade event). Associated words with entities also seems accurate in describing them. Note that Cosette was not detected in volume 3 because she is not explicitly cited (she is often refereed as "the daugther of M. Leblanc"), and this also explains the absence of the Cosette-Valjean pair. 

\section{Conclusion}
\label{conclusion}

In this article, we introduced a general framework in order to automatically extract textual information about narrative entities from a small corpus or a single work. The framework is build on two tables, the unit-word table $\mathbf{N}$ and the unit-entity table $\mathbf{E}$. This data organization sets subsequent analyses into a classical statistical framework, where the goal is to see how variables in $\mathbf{E}$ (the entities) affects the variables in $\mathbf{N}$ (the vocabulary) for each textual units. A choice was taken to use embeddings for analyzing these effects: units and words are embedded using Correspondence Analysis or pre-trained Word Embedding on $\mathbf{N}$, and entities are embedded in the same space as units using the Centroids or the Regressions methods on $\mathbf{E}$. These embeddings are then used in order to see affinities between entities and words, enabling the characterization of the formers by the latters. A case study on \emph{Les Misérables} was performed to see if methods gave promising results.

The first important choice in the analysis is how to define the size of units. Other corpora were also tested (e.g. Shakespeare plays) and it seems important to define units with at least a paragraph size (after preprocessing) in order to represent them accurately. Choosing small units might be able to successfully capture word specificities related to a small subset of entities, but unit vectors become almost orthogonal one to another if the size of units is too small. This situation results in an overfitting regime with an high variance and low bias, i.e. units positions can be very different with the difference of only a few rare words. By contrast, large units will result in an underfitting regime, with a low variance and high bias, failing to capture entities specificities, but more robust to particularities in word usages. Having enough units is also important in order to properly locate entities in the embedding space. In order to analyze characters and relationships, we advice the practitioner to use his prior knowledge of the work in order to split the studied narrative as close as possible to "scenes" (as found in theater), which describes a particular event between an almost constant set of characters.

The second choice is to select which entities to study. This choice is of course driven by the problematic, but is also limited by the automatic extraction tools available. These entities can be quite various, but must appear frequently in the work in order to be placed correctly in the embedding space. However, it is unadvised to set an entity which is almost always there (e.g. a narrator), as it will already be represented by the origin in the $\mathbf{CENT}$ method or as the constant term in the $\mathbf{REG}$ method. As a rule of thumbs, the number of entities should ideally be lower than the number of textual units. However, even with an exceeding number of entities (like in our case study, where we had 601 and 1124 entities for 365 units), if some entities appear rarely, analyses are still possible. Note that the version of the table $\mathbf{E}$ containing counts of entities rather than presence among each units was also tested in experiments, but gave similar results for the studied corpus.

The choice of using embeddings, where units, entities and words are located, is motivated by the fact that the resulting space permits many types of explorations. As presented in this article, we can extract some of the most (or least) associated words with each entity or rank entities according to a word query, but other types of measurements could also be made. Entities could be placed along a particular axis in the space, define with a set of positive and a set of negative words, in order to highlight a particular contrast (positive-negative, like in sentiment analysis, introvert-extrovert, friend-enemy, etc.). This approach could also be combined with a clustering of the words, or a Topic Modeling method, thus permitting to further refine the different regions in the embedding space. Relative location of entities could also be used in order to cluster or classify them. All these leads can be explored in future researches.

The difference in the choice between \textbf{CA} and \textbf{WV} embeddings appears quite clearly in the results. \textbf{CA} highlights particular words associated with entities, very specific to the studied work and the narrative events found in it, while \textbf{WV} gives a general feeling of the tone of the text when these entities are present. This difference is explained by the fact that \textbf{CA} focuses on words appearing within the work, with possibly very different locations to semantically similar words, while \textbf{WV} word vectors are positioned regarding their semantic and syntactic similarities. A entity located in the \textbf{WV} space will then be in a semantic or syntactic region, and its characterizing words should all be related. Results show that \textbf{CA} methods generally perform better to quickly interpret entities among the narrative, but might bit limited for some applications. As a matter of fact, the advantage of \textbf{WV} embeddings is that its space is absolute, permitting the comparison of results between sequentially studied texts, and also contains a larger vocabulary in its embedding. This last property could be used in order to use a fixed list of relationship attributes (e.g., friend, enemy, family, colleague), which do not necessarily appear in every text in the studied corpus, in order to categorized character relationships.

The choice between the \textbf{CENT} method and the \textbf{REG} method is relatively easy: thanks to its hyperparameter $\lambda$, the \textbf{REG} method can give similar results than the \textbf{CENT} method when $\lambda$ is high (with only a contraction of entity vectors), but also gives more "perpendicular" sets of words describing entities when $\lambda$ is low. Thus, it is clearly a superior choice in order to give a variety of results. The choice for this hyperparameter $\lambda$ depends on what the practitioner desire. If her/his entities are defined such that some of them are completely included into others, such as character and character pair, and she/he would like to have specificities about the finer grained entity, the $\lambda$ must be set to a low value. By contrast, if he do not mind in having some of her/his entities described like a mixture of others, she/he can set $\lambda$ to a high value. However, very low values of $\lambda$ should be avoided if the number of entities is high compared to the number of units, as this will lead to an overfitting of regression coefficients and result in the association of very rare and specific words with entities.

Finally, the biggest weakness of this framework yet is the difficulty to validate its pertinence. Several other case studies, with results carefully scrutinized regarding prior knowledge, should be undertaken in order to see if results are trustworthy, but this type of experiments are expansive both in time and human resources. Another idea could be to use annotated corpora such as described in \cite{Massey2015}, where human annotators classified character relationships in various categories. For example, we could see if the presented method can actually retrieve these categories by assigning relationships to the category-word with the highest association score. Such experiments are promising, but it requires an efficient automatic entity tagger, in order to detect and especially unify characters in a large quantity of documents, and unfortunately, this tool do not exist yet. Nevertheless, first case studies gave promising results for this framework, and its flexibility could lead to various applications.

\appendix

\section{Appendix}

\subsection{Correspondence Analysis}
\label{ca_details}

Starting from the $(n \times v)$ contingency table $\mathbf{N} = (n_{ij})$, we define the vector of unit weights as $\mathbf{f} = (f_i) := (n_{i\bullet}/n_{\bullet \bullet})$ and the vector of word weights as $\mathbf{g} = (g_j) := (n_{\bullet j}/n_{\bullet \bullet})$, where $\bullet$ denotes the summation on the replaced index. It is then possible to compute the \emph{weighted scalar product matrix between units} $\mathbf{K} = (k_{ij})$ with

\begin{equation}
k_{ij} := \sqrt{f_i f_j} \sum_{k=1}^{v} g_k(q_{ik} - 1)(q_{jk} - 1), 
\end{equation}

where $q_{ik} = \frac{n_{ik} n_{\bullet \bullet}}{n_{j \bullet} n_{\bullet k}}$ is the \emph{quotient of independence} of the cell $i, k$. The vector of textual unit $i$, $\mathbf{x}_i = (x_{i\alpha})$, is obtained by the eigendecomposition of the matrix $\mathbf{K} = \mathbf{U}\bm{\Lambda}\mathbf{U}^\top$ and with

\begin{equation}
x_{i\alpha} := \frac{\sqrt{\lambda_\alpha}}{\sqrt{f_i}} u_{i \alpha},
\end{equation}

where $\lambda_\alpha$ are the eigenvalues contained in the diagonal matrix $\bm{\Lambda}$ and $u_{i \alpha}$ the eigenvectors components found in $\mathbf{U}$. We find the vector of word $j$, $\mathbf{w}_j = (w_{j\alpha})$, with 

\begin{equation}
w_{j\alpha} := \frac{1}{\sqrt{\lambda_\alpha}} \sum_{i=1}^v f_i q_{ij} x_{i \alpha}.
\end{equation}

Note that various other quantities of interest can also be computed in CA, such as 
\begin{align*}
p_\alpha &:= \frac{\lambda_\alpha}{\lambda_\bullet} :\text{\small the proportion of inertia expressed in $\alpha$,} \\
c^u_{i \alpha} &:= \frac{f_i x_{i\alpha}^2}{\lambda_\alpha} : \text{\small the contribution of unit $i$ to axis $\alpha$,} \\
c^w_{j \alpha} &:= \frac{g_j w_{j\alpha}^2}{\lambda_\alpha} : \text{\small the contribution of word $j$ to axis $\alpha$,} \\
h^u_{i \alpha} &:= \frac{x_{i\alpha}^2}{\sum_{\alpha} x_{i\alpha}^2} : \text{\small the contribution of  axis $\alpha$ to unit $i$,} \\
h^w_{j \alpha} &:= \frac{w_{j\alpha}^2}{\sum_{\alpha} w_{j\alpha}^2} : \text{\small the contribution of  axis $\alpha$ to word $j$,}
\end{align*}
For a detailed interpretation of these different quantities, see \cite{Lebart2019}. 

\subsection{Unit embedding based of pre-trained word vectors}
\label{wv_details}

This methods is justified and detailed in \cite{Arora2017}. Let $\mathbf{w}_1, \ldots, \mathbf{w}_v$ be pre-trained word vectors which appear in the studied corpus, and the $(n \times v)$ table $\mathbf{N}$ counting the frequency of these words in the $n$ textual units. We first construct the \emph{uncentered vectors} $\widetilde{\mathbf{x}}_i$ of each unit $i$ with
\begin{equation}
\widetilde{\mathbf{x}}_i = \sum_{j = 1}^v \frac{n_{ij}}{n_{i \bullet}} \frac{a}{a + \frac{n_{\bullet j}}{n_{\bullet \bullet}}} \mathbf{w}_j,
\end{equation}
where $a > 0$ is an hyperparameter which gives less importance to frequent words as $a \to 0$. In this article, we set $a$ to the recommended value of $0.01$. Let $\widetilde{\mathbf{X}}$ be the matrix whose columns are vectors $\widetilde{\mathbf{x}}_i$, and $\mathbf{u}$ be its first singular vector. We compute \emph{vectors} $\mathbf{x}_i$ of each units $i$ with
\begin{equation}
\mathbf{x}_i = \widetilde{\mathbf{x}}_i - \mathbf{u}\mathbf{u}^\top \widetilde{\mathbf{x}}_i.
\end{equation}
This last equation act like a \emph{centration} of unit vectors in the direction of the first singular vector $\mathbf{u}$.

%% Define the bibliography file to be used
\bibliographystyle{apalike}
\bibliography{charnet}

\end{document}
